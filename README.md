Fine-Tuning DeepSeek Model with Unsloth

This repository provides a structured and efficient approach to fine-tuning DeepSeek models using Unsloth, Parameter-Efficient Fine-Tuning (PEFT), and Low-Rank Adaptation (LoRA). The Jupyter Notebook guides users through data preprocessing, model fine-tuning, and evaluation, leveraging memory-efficient techniques to optimize performance.
Key Features

âœ” Unsloth-Optimized Fine-Tuning â€“ Leverages Unsloth for efficient training on consumer hardware.
âœ” PEFT & LoRA Implementation â€“ Reduces computational overhead while maintaining model accuracy.
âœ” Custom Dataset Integration â€“ Supports dataset loading via Hugging Face Datasets for tailored model training.
âœ” Optimized for Cloud & Local Training â€“ Seamlessly runs on Google Colab or local GPU environments.
Requirements

    Python 3.8+
    PyTorch
    Hugging Face Transformers
    PEFT
    Unsloth

Usage

To get started, clone the repository and launch the Jupyter Notebook:

git clone https://github.com/sharrybhatti/deepseek-finetuning.git
cd deepseek-finetuning
jupyter notebook DeepSeek_Finetuning.ipynb

For inquiries, contributions, or improvements, feel free to open an issue or submit a pull request. ðŸš€
